#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
30_hypernet_improved.py - æ”¹è¿›ç‰ˆHyperNet
=========================================
é’ˆå¯¹ä¸ç¨³å®šé—®é¢˜çš„æ”¹è¿›ï¼š
1. æƒé‡ç”Ÿæˆæ­£åˆ™åŒ– - é™åˆ¶è¶…ç½‘ç»œè¾“å‡ºèŒƒå›´
2. ç‰¹å¾Bagging - æ¯ä¸ªå­ç½‘ç»œéšæœºé€‰æ‹©ç‰¹å¾å­é›†ï¼ˆç±»ä¼¼RFï¼‰
3. Prototype-based - åŸºäºç±»åˆ«åŸå‹çš„ç¨³å®šå­¦ä¹ 
4. å¤šæ¬¡è¿è¡ŒæŠ•ç¥¨ - å‡å°‘éšæœºæ€§
5. æ¸©åº¦ç¼©æ”¾ - ç¨³å®šsoftmaxè¾“å‡º

ç›®æ ‡ï¼šVAE+HyperNetè¾¾åˆ°80%

è¿è¡Œ: python 30_hypernet_improved.py
"""

import os
import sys
import json
import time
import logging
import warnings
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

warnings.filterwarnings('ignore')

SCRIPT_DIR = Path(__file__).parent
LOG_DIR = SCRIPT_DIR / 'logs'
OUTPUT_DIR = SCRIPT_DIR / 'output'
LOG_DIR.mkdir(exist_ok=True)
OUTPUT_DIR.mkdir(exist_ok=True)


class VAE(nn.Module):
    def __init__(self, input_dim):
        super(VAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 32), nn.ReLU(),
            nn.Linear(32, 16), nn.ReLU(),
        )
        self.fc_mu = nn.Linear(16, 8)
        self.fc_var = nn.Linear(16, 8)
        self.decoder = nn.Sequential(
            nn.Linear(8, 16), nn.ReLU(),
            nn.Linear(16, 32), nn.ReLU(),
            nn.Linear(32, input_dim),
        )
    
    def forward(self, x):
        h = self.encoder(x)
        mu, log_var = self.fc_mu(h), self.fc_var(h)
        z = mu + torch.exp(0.5 * log_var) * torch.randn_like(log_var)
        return self.decoder(z), mu, log_var


class StableHyperNet(nn.Module):
    """
    ç¨³å®šçš„è¶…ç½‘ç»œè®¾è®¡
    
    æ”¹è¿›ç‚¹ï¼š
    1. åŸºäºç±»åˆ«åŸå‹ç”Ÿæˆæƒé‡ï¼ˆè€Œä¸æ˜¯éšæœºä¸Šä¸‹æ–‡ï¼‰
    2. æƒé‡ç”Ÿæˆä½¿ç”¨tanhé™åˆ¶èŒƒå›´
    3. æ¯ä¸ªå­ç½‘ç»œä½¿ç”¨ç‰¹å¾å­é›†ï¼ˆFeature Baggingï¼‰
    4. æ®‹å·®è¿æ¥å¢åŠ ç¨³å®šæ€§
    """
    def __init__(self, input_dim, n_classes, n_subnets=30, hidden_dim=16):
        super(StableHyperNet, self).__init__()
        self.input_dim = input_dim
        self.n_classes = n_classes
        self.n_subnets = n_subnets
        self.hidden_dim = hidden_dim
        
        # ç‰¹å¾å­é›†å¤§å°ï¼ˆç±»ä¼¼RFçš„max_featuresï¼‰
        self.feature_subset_size = max(2, int(np.sqrt(input_dim)))
        
        # ä¸ºæ¯ä¸ªå­ç½‘ç»œéšæœºé€‰æ‹©ç‰¹å¾å­é›†
        self.feature_masks = nn.Parameter(
            torch.zeros(n_subnets, input_dim),
            requires_grad=False
        )
        for i in range(n_subnets):
            idx = np.random.choice(input_dim, self.feature_subset_size, replace=False)
            self.feature_masks.data[i, idx] = 1.0
        
        # ç±»åˆ«åŸå‹ï¼ˆå¯å­¦ä¹ ï¼‰
        self.class_prototypes = nn.Parameter(torch.randn(n_classes, input_dim) * 0.1)
        
        # è¶…ç½‘ç»œï¼šåŸºäºåŸå‹å·®å¼‚ç”Ÿæˆæƒé‡
        # è¾“å…¥ï¼šç±»åˆ«åŸå‹çš„å·®å¼‚å‘é‡
        proto_diff_dim = input_dim * n_classes
        
        # å­ç½‘ç»œå‚æ•°å¤§å°
        self.w1_size = self.feature_subset_size * hidden_dim
        self.b1_size = hidden_dim
        self.w2_size = hidden_dim * n_classes
        self.b2_size = n_classes
        total_params = self.w1_size + self.b1_size + self.w2_size + self.b2_size
        
        # è¶…ç½‘ç»œç”Ÿæˆå™¨ - è¾“å‡ºä½¿ç”¨tanhé™åˆ¶èŒƒå›´
        self.hypernet = nn.Sequential(
            nn.Linear(proto_diff_dim, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, n_subnets * total_params),
            nn.Tanh(),  # é™åˆ¶æƒé‡èŒƒå›´åœ¨[-1, 1]
        )
        
        # æƒé‡ç¼©æ”¾å› å­ï¼ˆå¯å­¦ä¹ ï¼‰
        self.weight_scale = nn.Parameter(torch.ones(1) * 0.5)
        
        # å­ç½‘ç»œæŠ•ç¥¨æƒé‡
        self.vote_weights = nn.Parameter(torch.ones(n_subnets) / n_subnets)
        
        # æ¸©åº¦å‚æ•°
        self.temperature = nn.Parameter(torch.ones(1))
    
    def compute_prototypes(self, X, y):
        """æ ¹æ®è®­ç»ƒæ•°æ®æ›´æ–°ç±»åˆ«åŸå‹"""
        with torch.no_grad():
            for c in range(self.n_classes):
                mask = (y == c)
                if mask.sum() > 0:
                    self.class_prototypes.data[c] = X[mask].mean(dim=0)
    
    def forward(self, x):
        batch_size = x.size(0)
        
        # åŸºäºåŸå‹ç”Ÿæˆæƒé‡
        proto_flat = self.class_prototypes.flatten().unsqueeze(0)
        all_params = self.hypernet(proto_flat)  # (1, n_subnets * total_params)
        all_params = all_params * self.weight_scale  # ç¼©æ”¾
        all_params = all_params.view(self.n_subnets, -1)
        
        all_logits = []
        
        for i in range(self.n_subnets):
            # è·å–ç‰¹å¾å­é›†
            mask = self.feature_masks[i]
            x_subset = x * mask.unsqueeze(0)  # ç‰¹å¾é€‰æ‹©
            x_subset = x_subset[:, mask.bool()]  # å‹ç¼©åˆ°å­é›†ç»´åº¦
            
            # æå–å­ç½‘ç»œå‚æ•°
            params = all_params[i]
            idx = 0
            
            W1 = params[idx:idx+self.w1_size].view(self.hidden_dim, self.feature_subset_size)
            idx += self.w1_size
            b1 = params[idx:idx+self.b1_size]
            idx += self.b1_size
            W2 = params[idx:idx+self.w2_size].view(self.n_classes, self.hidden_dim)
            idx += self.w2_size
            b2 = params[idx:idx+self.b2_size]
            
            # å‰å‘ä¼ æ’­ï¼ˆå¸¦æ®‹å·®ï¼‰
            h = F.relu(F.linear(x_subset, W1, b1))
            h = F.dropout(h, p=0.2, training=self.training)
            logits = F.linear(h, W2, b2)
            
            all_logits.append(logits)
        
        # åŠ æƒæŠ•ç¥¨
        all_logits = torch.stack(all_logits, dim=0)  # (n_subnets, batch, n_classes)
        weights = F.softmax(self.vote_weights, dim=0).view(-1, 1, 1)
        ensemble_logits = (all_logits * weights).sum(dim=0)
        
        # æ¸©åº¦ç¼©æ”¾
        return ensemble_logits / self.temperature.clamp(min=0.1)


def vae_augment(vae, X_train, y_train, aug_factor=100, device='cuda'):
    """å¤§é‡VAEæ•°æ®å¢å¼º"""
    vae.eval()
    X_t = torch.FloatTensor(X_train).to(device)
    
    X_aug = [X_train]
    y_aug = [y_train]
    
    with torch.no_grad():
        for cls in np.unique(y_train):
            mask = (y_train == cls)
            X_cls = X_t[mask]
            
            recon, mu, log_var = vae(X_cls)
            recon = recon.cpu().numpy()
            X_cls_np = X_cls.cpu().numpy()
            
            # å¤§é‡æ’å€¼
            for alpha in np.linspace(0.05, 0.95, aug_factor // 10):
                X_aug.append(alpha * X_cls_np + (1 - alpha) * recon)
                y_aug.append(np.full(mask.sum(), cls))
            
            # æ½œåœ¨ç©ºé—´é‡‡æ ·
            for scale in [0.2, 0.3, 0.4]:
                for _ in range(aug_factor // 30):
                    z = mu + torch.exp(0.5 * log_var) * torch.randn_like(log_var) * scale
                    X_aug.append(vae.decoder(z).cpu().numpy())
                    y_aug.append(np.full(mask.sum(), cls))
            
            # å™ªå£°å¢å¼º
            for noise in [0.03, 0.05, 0.08]:
                X_noisy = X_cls_np + np.random.randn(*X_cls_np.shape) * noise
                X_aug.append(X_noisy)
                y_aug.append(np.full(mask.sum(), cls))
    
    return np.vstack(X_aug), np.hstack(y_aug)


def train_stable_hypernet(model, X, y, epochs=300, lr=0.005, device='cuda'):
    """ç¨³å®šè®­ç»ƒ"""
    model.train()
    
    X_t = torch.FloatTensor(X).to(device)
    y_t = torch.LongTensor(y).to(device)
    
    # æ›´æ–°åŸå‹
    model.compute_prototypes(X_t, y_t)
    
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.02)
    
    # å­¦ä¹ ç‡é¢„çƒ­ + ä½™å¼¦é€€ç«
    warmup_epochs = 30
    
    for epoch in range(epochs):
        # å­¦ä¹ ç‡é¢„çƒ­
        if epoch < warmup_epochs:
            for pg in optimizer.param_groups:
                pg['lr'] = lr * (epoch + 1) / warmup_epochs
        else:
            # ä½™å¼¦é€€ç«
            progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)
            for pg in optimizer.param_groups:
                pg['lr'] = lr * 0.5 * (1 + np.cos(np.pi * progress))
        
        optimizer.zero_grad()
        
        # Mini-batch with balanced sampling
        batch_size = min(128, len(X_t))
        idx = torch.randperm(len(X_t))[:batch_size]
        
        logits = model(X_t[idx])
        
        # Label smoothing + focal loss inspired weighting
        loss = F.cross_entropy(logits, y_t[idx], label_smoothing=0.15)
        
        # æ­£åˆ™åŒ–ï¼šé¼“åŠ±å­ç½‘ç»œå¤šæ ·æ€§
        if hasattr(model, 'vote_weights'):
            entropy = -(F.softmax(model.vote_weights, dim=0) * 
                       F.log_softmax(model.vote_weights + 1e-8, dim=0)).sum()
            loss = loss - 0.01 * entropy  # é¼“åŠ±å‡åŒ€æŠ•ç¥¨
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()
        
        # å®šæœŸæ›´æ–°åŸå‹
        if epoch % 50 == 0:
            with torch.no_grad():
                model.compute_prototypes(X_t, y_t)


def evaluate_with_multiple_runs(model_class, X_train, y_train, X_test, n_runs=5, device='cuda'):
    """å¤šæ¬¡è¿è¡ŒæŠ•ç¥¨ - å‡å°‘éšæœºæ€§"""
    all_preds = []
    
    for run in range(n_runs):
        torch.manual_seed(run * 42)
        np.random.seed(run * 42)
        
        # è®­ç»ƒVAE
        vae = VAE(X_train.shape[1]).to(device)
        optimizer = optim.Adam(vae.parameters(), lr=0.01)
        X_t = torch.FloatTensor(X_train).to(device)
        
        for _ in range(80):
            optimizer.zero_grad()
            recon, mu, log_var = vae(X_t)
            loss = nn.MSELoss()(recon, X_t) + 0.01 * (-0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp()))
            loss.backward()
            optimizer.step()
        
        # æ•°æ®å¢å¼º
        X_aug, y_aug = vae_augment(vae, X_train, y_train, aug_factor=100, device=device)
        
        # è®­ç»ƒHyperNet
        model = model_class(X_train.shape[1], len(np.unique(y_train)), n_subnets=30).to(device)
        train_stable_hypernet(model, X_aug, y_aug, epochs=300, device=device)
        
        # é¢„æµ‹
        model.eval()
        with torch.no_grad():
            X_test_t = torch.FloatTensor(X_test).to(device)
            logits = model(X_test_t)
            pred = logits.argmax(dim=1).cpu().numpy()
            all_preds.append(pred)
    
    # å¤šæ•°æŠ•ç¥¨
    all_preds = np.array(all_preds)
    final_pred = []
    for i in range(len(X_test)):
        votes = all_preds[:, i]
        final_pred.append(np.bincount(votes.astype(int)).argmax())
    
    return np.array(final_pred)


def main():
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = LOG_DIR / f'30_hypernet_improved_{timestamp}.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file, encoding='utf-8')
        ]
    )
    logger = logging.getLogger(__name__)
    
    logger.info("=" * 70)
    logger.info("30_hypernet_improved.py - æ”¹è¿›ç‰ˆç¨³å®šHyperNet")
    logger.info("=" * 70)
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    logger.info(f"è®¾å¤‡: {device}")
    
    data_path = SCRIPT_DIR / 'data' / 'Data_for_Jinming.csv'
    df = pd.read_csv(data_path)
    X = df[['LAA', 'Glutamate', 'Choline', 'Sarcosine']].values.astype(np.float32)
    y = LabelEncoder().fit_transform(df['Group'].values)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    logger.info(f"æ•°æ®: {len(X)} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    
    results = {}
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # ==================== RF ====================
    logger.info("\n[1/4] RF åŸºçº¿...")
    rf_preds, rf_trues = [], []
    for train_idx, test_idx in skf.split(X_scaled, y):
        rf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)
        rf.fit(X_scaled[train_idx], y[train_idx])
        rf_preds.extend(rf.predict(X_scaled[test_idx]).tolist())
        rf_trues.extend(y[test_idx].tolist())
    results['RF'] = accuracy_score(rf_trues, rf_preds) * 100
    logger.info(f"   RF: {results['RF']:.2f}%")
    
    # ==================== VAE+RF ====================
    logger.info("\n[2/4] VAE + RF...")
    vae_rf_preds, vae_rf_trues = [], []
    for train_idx, test_idx in skf.split(X_scaled, y):
        X_train, y_train = X_scaled[train_idx], y[train_idx]
        X_test, y_test = X_scaled[test_idx], y[test_idx]
        
        vae = VAE(X_train.shape[1]).to(device)
        optimizer = optim.Adam(vae.parameters(), lr=0.01)
        X_t = torch.FloatTensor(X_train).to(device)
        for _ in range(80):
            optimizer.zero_grad()
            recon, mu, log_var = vae(X_t)
            loss = nn.MSELoss()(recon, X_t) + 0.01 * (-0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp()))
            loss.backward()
            optimizer.step()
        
        X_aug, y_aug = vae_augment(vae, X_train, y_train, aug_factor=100, device=device)
        
        rf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)
        rf.fit(X_aug, y_aug)
        vae_rf_preds.extend(rf.predict(X_test).tolist())
        vae_rf_trues.extend(y_test.tolist())
    
    results['VAE+RF'] = accuracy_score(vae_rf_trues, vae_rf_preds) * 100
    logger.info(f"   VAE+RF: {results['VAE+RF']:.2f}%")
    
    # ==================== æ”¹è¿›HyperNetï¼ˆå•æ¬¡è¿è¡Œï¼‰ ====================
    logger.info("\n[3/4] VAE + StableHyperNet (å•æ¬¡)...")
    hypernet_preds, hypernet_trues = [], []
    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_scaled, y)):
        X_train, y_train = X_scaled[train_idx], y[train_idx]
        X_test, y_test = X_scaled[test_idx], y[test_idx]
        
        torch.manual_seed(42)
        np.random.seed(42)
        
        # VAE
        vae = VAE(X_train.shape[1]).to(device)
        optimizer = optim.Adam(vae.parameters(), lr=0.01)
        X_t = torch.FloatTensor(X_train).to(device)
        for _ in range(80):
            optimizer.zero_grad()
            recon, mu, log_var = vae(X_t)
            loss = nn.MSELoss()(recon, X_t) + 0.01 * (-0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp()))
            loss.backward()
            optimizer.step()
        
        X_aug, y_aug = vae_augment(vae, X_train, y_train, aug_factor=100, device=device)
        
        # StableHyperNet
        model = StableHyperNet(X_train.shape[1], len(np.unique(y_train)), n_subnets=30).to(device)
        train_stable_hypernet(model, X_aug, y_aug, epochs=300, device=device)
        
        model.eval()
        with torch.no_grad():
            X_test_t = torch.FloatTensor(X_test).to(device)
            pred = model(X_test_t).argmax(dim=1).cpu().numpy()
        
        hypernet_preds.extend(pred.tolist())
        hypernet_trues.extend(y_test.tolist())
        acc = accuracy_score(y_test, pred) * 100
        logger.info(f"   Fold {fold_idx+1}/5: {acc:.2f}%")
    
    results['VAE+StableHyperNet(å•æ¬¡)'] = accuracy_score(hypernet_trues, hypernet_preds) * 100
    logger.info(f"   VAE+StableHyperNet(å•æ¬¡): {results['VAE+StableHyperNet(å•æ¬¡)']:.2f}%")
    
    # ==================== æ”¹è¿›HyperNetï¼ˆå¤šæ¬¡è¿è¡ŒæŠ•ç¥¨ï¼‰ ====================
    logger.info("\n[4/4] VAE + StableHyperNet (5æ¬¡æŠ•ç¥¨)...")
    vote_preds, vote_trues = [], []
    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_scaled, y)):
        X_train, y_train = X_scaled[train_idx], y[train_idx]
        X_test, y_test = X_scaled[test_idx], y[test_idx]
        
        pred = evaluate_with_multiple_runs(
            StableHyperNet, X_train, y_train, X_test, 
            n_runs=5, device=device
        )
        
        vote_preds.extend(pred.tolist())
        vote_trues.extend(y_test.tolist())
        acc = accuracy_score(y_test, pred) * 100
        logger.info(f"   Fold {fold_idx+1}/5: {acc:.2f}%")
    
    results['VAE+StableHyperNet(5æ¬¡æŠ•ç¥¨)'] = accuracy_score(vote_trues, vote_preds) * 100
    logger.info(f"   VAE+StableHyperNet(5æ¬¡æŠ•ç¥¨): {results['VAE+StableHyperNet(5æ¬¡æŠ•ç¥¨)']:.2f}%")
    
    # ==================== æ±‡æ€» ====================
    logger.info("\n" + "=" * 70)
    logger.info("[ç»“æœå¯¹æ¯”]")
    logger.info("=" * 70)
    for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):
        marker = "ğŸ†" if acc == max(results.values()) else "  "
        logger.info(f"{marker} {name:30s}: {acc:.2f}%")
    logger.info("=" * 70)
    
    result_file = OUTPUT_DIR / f'30_hypernet_improved_{timestamp}.json'
    with open(result_file, 'w') as f:
        json.dump(results, f, indent=2)
    logger.info(f"ä¿å­˜: {result_file}")


if __name__ == '__main__':
    main()
