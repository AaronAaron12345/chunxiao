#!/usr/bin/env python3
"""
VAE-HyperNet-Fusion (VAE-HNF) Model

A generative framework for small tabular data classification via
hypernetwork-generated soft decision trees.

Architecture:
    Input -> VAE Augmentation -> Multi-Head HyperNetwork -> Soft Decision
    Tree Ensemble -> Classification

Components:
    1. VAE: Learns the latent data distribution and generates synthetic
       samples via interpolation for data augmentation.
    2. Multi-Head HyperNetwork: Encodes the augmented training set into a
       compact context representation, then generates all parameters of the
       target classification network through multiple independent heads.
    3. Soft Decision Tree Ensemble: A differentiable ensemble of soft
       decision trees parameterized by the hypernetwork output.
"""

import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score


# ---------------------------------------------------------------
# Reproducibility
# ---------------------------------------------------------------
def set_seed(seed: int = 42):
    """Set random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# ---------------------------------------------------------------
# 1. Variational Autoencoder (VAE)
# ---------------------------------------------------------------
class VAE(nn.Module):
    """Variational autoencoder for tabular data augmentation.

    The encoder maps input features to a latent Gaussian distribution.
    The decoder reconstructs data from latent samples.  During data
    augmentation, new samples are generated by perturbing latent codes
    with controlled noise, followed by decoding.

    Args:
        input_dim:  Number of input features.
        latent_dim: Dimensionality of the latent space.
    """

    def __init__(self, input_dim: int, latent_dim: int = 8):
        super().__init__()
        hidden = 32

        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden),
            nn.LayerNorm(hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(hidden, latent_dim)
        self.fc_logvar = nn.Linear(hidden, latent_dim)

        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, input_dim),
        )

        # Orthogonal initialization for stable training
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def encode(self, x: torch.Tensor):
        """Encode input into latent mean and log-variance."""
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor,
                       noise_scale: float = 1.0) -> torch.Tensor:
        """Sample from the latent distribution using reparameterization."""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std * noise_scale

    def forward(self, x: torch.Tensor, noise_scale: float = 1.0):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar, noise_scale)
        return self.decoder(z), mu, logvar


# ---------------------------------------------------------------
# 2. Multi-Head Hypernetwork
# ---------------------------------------------------------------
class HyperNetwork(nn.Module):
    """Multi-head hypernetwork that generates parameters for the target
    soft decision tree ensemble.

    The hypernetwork first encodes the training data into a shared context
    vector via mean pooling of per-sample embeddings.  Multiple independent
    generator heads then produce candidate parameter vectors, which are
    combined via learned softmax-weighted averaging.  The merged output
    is passed through tanh to constrain the weight magnitude.

    Args:
        input_dim:  Number of input features.
        n_classes:  Number of target classes.
        n_trees:    Number of soft decision trees in the ensemble.
        tree_depth: Depth of each tree.
        hidden_dim: Hidden layer width.
        n_heads:    Number of independent generator heads.
        dropout:    Dropout rate for regularization.
    """

    def __init__(self, input_dim: int, n_classes: int, n_trees: int = 15,
                 tree_depth: int = 3, hidden_dim: int = 64,
                 n_heads: int = 5, dropout: float = 0.15):
        super().__init__()
        self.input_dim = input_dim
        self.n_trees = n_trees
        self.tree_depth = tree_depth
        self.n_heads = n_heads
        self.n_classes = n_classes

        n_internal = 2 ** tree_depth - 1
        n_leaves = 2 ** tree_depth
        self.n_internal = n_internal
        self.n_leaves = n_leaves

        # Parameters per tree: split weights + split biases + leaf logits
        self.params_per_tree = (n_internal * input_dim
                                + n_internal
                                + n_leaves * n_classes)
        # Total: per-tree params × n_trees + tree combination weights
        self.total_params = self.params_per_tree * n_trees + n_trees

        # Shared data encoder
        self.data_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout * 0.67),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )

        # Independent generator heads
        self.generator_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim * 2),
                nn.LayerNorm(hidden_dim * 2),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim * 2, hidden_dim * 2),
                nn.ReLU(),
                nn.Dropout(dropout * 0.67),
                nn.Linear(hidden_dim * 2, self.total_params),
            )
            for _ in range(n_heads)
        ])

        # Learnable head combination weights
        self.head_weights = nn.Parameter(torch.ones(n_heads) / n_heads)
        # Learnable output scale
        self.output_scale = nn.Parameter(torch.tensor(1.0))

        # Orthogonal initialization
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=0.5)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Generate target network parameters from training data.

        Args:
            x: Training data tensor of shape (n_samples, input_dim).

        Returns:
            Parameter vector of shape (total_params,).
        """
        # Encode and aggregate via mean pooling
        context = self.data_encoder(x).mean(dim=0)

        # Generate parameters from each head
        head_outputs = torch.stack([head(context)
                                    for head in self.generator_heads])

        # Weighted combination with softmax-normalized weights
        weights = F.softmax(self.head_weights, dim=0)
        params = torch.einsum('h,hp->p', weights, head_outputs)

        # Constrain parameter magnitude
        return torch.tanh(params) * self.output_scale

    def parse_params(self, params: torch.Tensor):
        """Parse the flat parameter vector into per-tree weight dicts.

        Returns:
            trees_params: List of dicts with keys
                'split_weights', 'split_bias', 'leaf_logits'.
            tree_weights: Tensor of shape (n_trees,) for ensemble
                combination.
        """
        trees_params = []
        offset = 0

        for _ in range(self.n_trees):
            sw_size = self.n_internal * self.input_dim
            split_weights = params[offset:offset + sw_size].view(
                self.n_internal, self.input_dim)
            offset += sw_size

            split_bias = params[offset:offset + self.n_internal]
            offset += self.n_internal

            ll_size = self.n_leaves * self.n_classes
            leaf_logits = params[offset:offset + ll_size].view(
                self.n_leaves, self.n_classes)
            offset += ll_size

            trees_params.append({
                'split_weights': split_weights,
                'split_bias': split_bias,
                'leaf_logits': leaf_logits,
            })

        tree_weights = params[offset:offset + self.n_trees]
        return trees_params, tree_weights


# ---------------------------------------------------------------
# 3. Soft Decision Tree Ensemble
# ---------------------------------------------------------------
class SoftDecisionTreeEnsemble(nn.Module):
    """Differentiable ensemble of soft decision trees.

    Each internal node computes a soft split via sigmoid, producing
    path probabilities to all leaf nodes.  Leaf nodes hold class-
    probability distributions (softmax-normalized logits).  The
    temperature parameter is learnable and shared across all trees.

    Args:
        tree_depth: Depth of each tree.
        init_temp:  Initial temperature for soft splits.
    """

    def __init__(self, tree_depth: int = 3, init_temp: float = 1.0):
        super().__init__()
        self.depth = tree_depth
        self.n_internal = 2 ** tree_depth - 1
        self.n_leaves = 2 ** tree_depth
        self.log_temperature = nn.Parameter(
            torch.tensor(float(np.log(init_temp))))

    @property
    def temperature(self) -> torch.Tensor:
        """Learnable temperature clamped to [0.1, 5.0]."""
        return torch.exp(self.log_temperature).clamp(0.1, 5.0)

    def _forward_single_tree(self, x: torch.Tensor,
                             split_weights: torch.Tensor,
                             split_bias: torch.Tensor,
                             leaf_logits: torch.Tensor) -> torch.Tensor:
        """Forward pass through a single soft decision tree."""
        batch_size = x.size(0)

        # Soft split probabilities at internal nodes
        split_probs = torch.sigmoid(
            (x @ split_weights.T + split_bias) / self.temperature
        )

        # Compute path probability to each leaf
        leaf_probs = torch.ones(batch_size, self.n_leaves, device=x.device)
        for leaf_idx in range(self.n_leaves):
            path_prob = torch.ones(batch_size, device=x.device)
            node_idx = leaf_idx + self.n_internal
            for _ in range(self.depth):
                parent_idx = (node_idx - 1) // 2
                is_right = (node_idx % 2 == 0)
                if is_right:
                    path_prob = path_prob * split_probs[:, parent_idx]
                else:
                    path_prob = path_prob * (1 - split_probs[:, parent_idx])
                node_idx = parent_idx
            leaf_probs[:, leaf_idx] = path_prob

        # Weighted sum of leaf class distributions
        leaf_class_probs = F.softmax(
            leaf_logits / self.temperature, dim=-1)
        return torch.einsum('bl,lc->bc', leaf_probs, leaf_class_probs)

    def forward(self, x: torch.Tensor, trees_params: list,
                tree_weights: torch.Tensor) -> torch.Tensor:
        """Ensemble prediction by combining all trees.

        Args:
            x:             Input tensor (batch_size, input_dim).
            trees_params:  List of per-tree parameter dicts.
            tree_weights:  Tensor of tree importance weights.

        Returns:
            Class probability tensor (batch_size, n_classes).
        """
        outputs = torch.stack([
            self._forward_single_tree(
                x, tp['split_weights'], tp['split_bias'], tp['leaf_logits'])
            for tp in trees_params
        ])
        weights = F.softmax(tree_weights, dim=0)
        return torch.einsum('t,tbc->bc', weights, outputs)


# ---------------------------------------------------------------
# 4. Complete VAE-HyperNet-Fusion Model
# ---------------------------------------------------------------
class VAEHyperNetFusion(nn.Module):
    """VAE-HyperNet-Fusion: complete model combining VAE augmentation,
    multi-head hypernetwork, and soft decision tree ensemble.

    Args:
        input_dim:  Number of input features.
        n_classes:  Number of target classes.
        n_trees:    Number of trees in the ensemble.
        tree_depth: Depth of each decision tree.
        n_heads:    Number of hypernetwork generator heads.
        hidden_dim: Hidden layer width in the hypernetwork.
        latent_dim: VAE latent space dimensionality.
        dropout:    Dropout rate.
    """

    def __init__(self, input_dim: int, n_classes: int = 2,
                 n_trees: int = 15, tree_depth: int = 3,
                 n_heads: int = 5, hidden_dim: int = 64,
                 latent_dim: int = 8, dropout: float = 0.15):
        super().__init__()
        self.vae = VAE(input_dim, latent_dim=latent_dim)
        self.hypernet = HyperNetwork(
            input_dim, n_classes, n_trees, tree_depth,
            hidden_dim, n_heads, dropout)
        self.classifier = SoftDecisionTreeEnsemble(tree_depth)

    def augment(self, X: torch.Tensor, y: torch.Tensor,
                n_augment: int = 200,
                noise_scale: float = 0.3):
        """Generate augmented data deterministically using the trained VAE.

        Cycles through existing samples, encodes each into the latent
        space, adds scaled Gaussian noise, and decodes to produce a new
        synthetic sample.

        Args:
            X: Training features (n_samples, input_dim).
            y: Training labels (n_samples,).
            n_augment: Number of synthetic samples to generate.
            noise_scale: Scale of latent noise.

        Returns:
            Augmented feature and label tensors.
        """
        self.vae.eval()
        aug_X, aug_y = [X], [y]

        with torch.no_grad():
            for i in range(n_augment):
                idx = i % X.size(0)
                mu, logvar = self.vae.encode(X[idx:idx + 1])
                z = self.vae.reparameterize(mu, logvar, noise_scale)
                aug_X.append(self.vae.decoder(z))
                aug_y.append(y[idx:idx + 1])

        return torch.cat(aug_X), torch.cat(aug_y)

    def forward(self, X_train: torch.Tensor, X_test: torch.Tensor):
        """Forward pass: generate parameters from training data, then
        classify test data.

        Args:
            X_train: Training data for hypernetwork context.
            X_test:  Data to classify.

        Returns:
            output: Class probabilities (batch_size, n_classes).
            params: Generated parameter vector.
        """
        params = self.hypernet(X_train)
        trees_params, tree_weights = self.hypernet.parse_params(params)
        output = self.classifier(X_test, trees_params, tree_weights)
        return output, params


# ---------------------------------------------------------------
# 5. Training Procedure
# ---------------------------------------------------------------
DEFAULT_HYPERPARAMS = {
    'n_trees': 15,
    'tree_depth': 3,
    'n_heads': 5,
    'hidden_dim': 64,
    'latent_dim': 8,
    'dropout': 0.15,
    'n_augment': 200,
    'noise_scale': 0.3,
    'vae_lr': 0.002,
    'vae_epochs': 100,
    'kl_weight': 0.01,
    'lr': 0.01,
    'weight_decay': 0.05,
    'epochs': 300,
    'warmup_epochs': 20,
    'reg_weight': 0.01,
}


def train_model(X_train: np.ndarray, y_train: np.ndarray,
                X_val: np.ndarray, y_val: np.ndarray,
                n_classes: int, device: torch.device,
                hp: dict = None, seed: int = 42) -> VAEHyperNetFusion:
    """Train the VAE-HyperNet-Fusion model.

    Training proceeds in two phases:
      Phase 1 — Train the VAE on training data (reconstruction + KL loss).
      Phase 2 — Generate augmented data with the trained VAE, then train
                the hypernetwork and tree classifier with warmup + cosine
                annealing, consistency regularization, and early stopping.

    Args:
        X_train: Training features (n_train, d).
        y_train: Training labels (n_train,).
        X_val:   Validation features (n_val, d).
        y_val:   Validation labels (n_val,).
        n_classes: Number of classes.
        device:  Torch device.
        hp:      Hyperparameter dict (uses defaults if None).
        seed:    Random seed.

    Returns:
        Trained model (best validation state).
    """
    if hp is None:
        hp = DEFAULT_HYPERPARAMS.copy()

    set_seed(seed)
    input_dim = X_train.shape[1]

    model = VAEHyperNetFusion(
        input_dim, n_classes,
        n_trees=hp['n_trees'],
        tree_depth=hp['tree_depth'],
        n_heads=hp['n_heads'],
        hidden_dim=hp['hidden_dim'],
        latent_dim=hp['latent_dim'],
        dropout=hp['dropout'],
    ).to(device)

    X_t = torch.FloatTensor(X_train).to(device)
    y_t = torch.LongTensor(y_train).to(device)
    X_v = torch.FloatTensor(X_val).to(device)

    # Phase 1: Train VAE
    vae_opt = torch.optim.Adam(model.vae.parameters(),
                               lr=hp['vae_lr'], weight_decay=1e-5)
    model.vae.train()
    for _ in range(hp['vae_epochs']):
        vae_opt.zero_grad()
        recon, mu, logvar = model.vae(X_t)
        recon_loss = F.mse_loss(recon, X_t)
        kl_loss = -0.5 * torch.mean(
            1 + logvar - mu.pow(2) - logvar.exp())
        loss = recon_loss + hp['kl_weight'] * kl_loss
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.vae.parameters(), 1.0)
        vae_opt.step()

    # Generate augmented data (deterministic, one-time)
    model.vae.eval()
    with torch.no_grad():
        X_aug, y_aug = model.augment(
            X_t, y_t, hp['n_augment'], hp['noise_scale'])

    # Phase 2: Train HyperNetwork + Classifier
    params_to_train = (list(model.hypernet.parameters())
                       + list(model.classifier.parameters()))
    optimizer = torch.optim.AdamW(
        params_to_train, lr=hp['lr'], weight_decay=hp['weight_decay'])

    warmup = hp['warmup_epochs']
    epochs = hp['epochs']
    scheduler = torch.optim.lr_scheduler.LambdaLR(
        optimizer,
        lr_lambda=lambda e: (
            (e + 1) / warmup if e < warmup
            else 0.5 * (1 + np.cos(
                np.pi * (e - warmup) / max(epochs - warmup, 1)))
        ),
    )

    best_acc, best_state, no_improve = 0.0, None, 0

    for epoch in range(epochs):
        model.hypernet.train()
        model.classifier.train()
        optimizer.zero_grad()

        output, p = model(X_aug, X_aug)
        loss = F.cross_entropy(output, y_aug)
        loss = loss + hp['reg_weight'] * (p ** 2).mean()

        # Consistency regularization after warmup
        if epoch > warmup:
            model.eval()
            with torch.no_grad():
                output2, _ = model(X_aug, X_aug)
            model.hypernet.train()
            model.classifier.train()
            loss = loss + 0.1 * F.mse_loss(output, output2.detach())

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()
        scheduler.step()

        # Validation every 10 epochs
        if (epoch + 1) % 10 == 0:
            model.eval()
            with torch.no_grad():
                val_out, _ = model(X_t, X_v)
                val_acc = accuracy_score(
                    y_val, val_out.argmax(1).cpu().numpy())
            if val_acc > best_acc:
                best_acc = val_acc
                best_state = {k: v.clone()
                              for k, v in model.state_dict().items()}
                no_improve = 0
            else:
                no_improve += 1
            if no_improve >= 5:
                break

    if best_state is not None:
        model.load_state_dict(best_state)
    return model


def predict_model(model: VAEHyperNetFusion,
                  X_train: np.ndarray,
                  X_test: np.ndarray,
                  device: torch.device) -> np.ndarray:
    """Predict class probabilities for test data.

    Args:
        model:   Trained VAE-HNF model.
        X_train: Training data (for hypernetwork context).
        X_test:  Test data.
        device:  Torch device.

    Returns:
        Probability array of shape (n_test, n_classes).
    """
    model.eval()
    with torch.no_grad():
        output, _ = model(
            torch.FloatTensor(X_train).to(device),
            torch.FloatTensor(X_test).to(device))
        return F.softmax(output, dim=1).cpu().numpy()
